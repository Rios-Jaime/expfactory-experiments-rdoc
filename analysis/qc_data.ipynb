{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stopping_data(df, split_by_block_num=False):\n",
    "    \"\"\"\n",
    "    Extracts and calculates metrics related to 'stop' and 'go' conditions for test trials.\n",
    "    \n",
    "    The function processes data to compute key metrics, including accuracy, response time (rt),\n",
    "    stop signal delay (SSD), and omission rates, for both 'stop' and 'go' trial conditions.\n",
    "    The results can be grouped by block numbers if required.\n",
    "    \n",
    "    Input:\n",
    "      df: DataFrame containing task data for a specific task for a single subject.\n",
    "      split_by_block_num (optional): Boolean flag to determine if results should be grouped \n",
    "      by block number (default is False).\n",
    "      \n",
    "    Output:\n",
    "      Prints the computed metrics either grouped by block numbers or in an aggregated form.\n",
    "      \n",
    "    Metrics Calculated:\n",
    "      - stop_acc: Mean accuracy for 'stop' trials.\n",
    "      - go_acc: Mean accuracy for 'go' trials.\n",
    "      - avg_go_rt: Average response time for correct 'go' trials.\n",
    "      - max_SSD: Maximum stop signal delay.\n",
    "      - min_SSD: Minimum stop signal delay.\n",
    "      - mean_SSD: Average stop signal delay.\n",
    "      - stop_success: Percentage of successful stops.\n",
    "      - stop_fail: Percentage of failed stops.\n",
    "      - go_success: Percentage of successful 'go' trials.\n",
    "      - stop_omission_rate: Omission rate for 'stop' trials.\n",
    "      - go_omission_rate: Omission rate for 'go' trials.\n",
    "    \"\"\"\n",
    "    test_trials__df = df[(df['trial_id'] == 'test_trial')]\n",
    "    \n",
    "    grouping_column = 'block_num' if split_by_block_num else None\n",
    "\n",
    "    # If we're splitting by block_num, group the data by block_num\n",
    "    if split_by_block_num:\n",
    "        stop_trials = test_trials__df[(test_trials__df['condition'] == 'stop')].groupby(grouping_column)\n",
    "        go_trials = test_trials__df[(test_trials__df['condition'] == 'go')].groupby(grouping_column)\n",
    "    else:\n",
    "        stop_trials = test_trials__df[(test_trials__df['condition'] == 'stop')]\n",
    "        go_trials = test_trials__df[(test_trials__df['condition'] == 'go')]\n",
    "\n",
    "    # Define a helper function to calculate metrics for a given group\n",
    "    def calculate_metrics(group):\n",
    "        stop_acc = group[group['condition'] == 'stop']['stop_acc'].mean()\n",
    "        go_acc = group[group['condition'] == 'go']['go_acc'].mean()\n",
    "\n",
    "        go_correct_trials = group[(group['condition'] == 'go') & (group['go_acc'] == 1)]\n",
    "        avg_go_rt = go_correct_trials['rt'].mean()\n",
    "\n",
    "        max_SSD = group['SSD'].max()\n",
    "        min_SSD = group['SSD'].min()\n",
    "        mean_SSD = group['SSD'].mean()\n",
    "\n",
    "        stop_success = group[group['condition'] == 'stop']['stop_acc'].mean()\n",
    "        stop_fail = 1 - stop_success\n",
    "\n",
    "        go_success = group[group['condition'] == 'go']['go_acc'].mean()\n",
    "        stop_omission_rate = group[group['condition'] == 'stop']['rt'].isna().mean()\n",
    "        go_omission_rate = group[group['condition'] == 'go']['rt'].isna().mean()\n",
    "\n",
    "        return {\n",
    "            \"stop_acc\": stop_acc,\n",
    "            \"go_acc\": go_acc,\n",
    "            \"avg_go_rt\": avg_go_rt,\n",
    "            \"max_SSD\": max_SSD,\n",
    "            \"min_SSD\": min_SSD,\n",
    "            \"mean_SSD\": mean_SSD,\n",
    "            \"stop_success\": stop_success,\n",
    "            \"stop_fail\": stop_fail,\n",
    "            \"go_success\": go_success,\n",
    "            \"stop_omission_rate\": stop_omission_rate,\n",
    "            \"go_omission_rate\": go_omission_rate\n",
    "        }\n",
    "\n",
    "    # If we're splitting by block_num, apply the helper function to each group\n",
    "    if split_by_block_num:\n",
    "        results = test_trials__df.groupby(grouping_column).apply(calculate_metrics)\n",
    "        print(results)\n",
    "    else:\n",
    "        results = calculate_metrics(test_trials__df)\n",
    "        for key, value in results.items():\n",
    "            print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_attention_check_accuracy(df):\n",
    "    \"\"\"\n",
    "    Calculates the attention check accuracy for attention checks\n",
    "    \n",
    "    This function computes the accuracy for a given set of attention checks for a single task df.\n",
    "    \n",
    "    Input:\n",
    "      df: DataFrame containing task data for a specific task for a single subject.  \n",
    "          \n",
    "    Output:\n",
    "      Prints the overall attention check accuracy for a given task df for a single subject. \n",
    "    \"\"\"\n",
    "\n",
    "    test_trials__df = df[(df['trial_id'] == 'test_attention_check')]\n",
    "    attention_check_accuracy = test_trials__df['correct_trial'].mean()\n",
    "    print(attention_check_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_rt(df, condition_col='condition', test_trial='test_trial', correct_trial_col='correct_trial', factorial_condition=False, factorial_conditions=['cue_condition', 'task_condition'], split_by_block_num=False):\n",
    "    \"\"\"\n",
    "    Calculates the average reaction time (RT) for given test trials based on specific conditions.\n",
    "    \n",
    "    This function can handle both standard conditions and factorial conditions. Additionally,\n",
    "    results can optionally be split by block number.\n",
    "    \n",
    "    Input:\n",
    "      df: DataFrame containing the task data.\n",
    "      condition_col: Name of the column representing the condition. Default is 'condition'.\n",
    "      test_trial: Name of the column indicating the type of trial. Default is 'test_trial'.\n",
    "      correct_trial_col: Column indicating if the trial was correctly executed. Default is 'correct_trial'.\n",
    "      factorial_condition: Boolean to specify if the data has factorial conditions. Default is False.\n",
    "      factorial_conditions: List of columns indicating factorial conditions. Default is ['cue_condition', 'task_condition'].\n",
    "      split_by_block_num: Boolean to specify if results should be split by block number. Default is False.\n",
    "    \n",
    "    Output:\n",
    "      Prints the average RT for the specified conditions.\n",
    "    \"\"\"    \n",
    "    test_trials__df = df[(df['trial_id'] == test_trial) & (df[correct_trial_col] == 1)]\n",
    "    \n",
    "    if factorial_condition:\n",
    "        grouping_columns = factorial_conditions\n",
    "    else:\n",
    "        grouping_columns = [condition_col]\n",
    "    \n",
    "    if split_by_block_num:\n",
    "        grouping_columns.append('block_num')\n",
    "    \n",
    "    rt_by_condition = test_trials__df.groupby(grouping_columns).apply(lambda group: group['rt'].mean())\n",
    "    print(rt_by_condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_omission_rate(df, test_trial='test_trial', condition_col='condition', factorial_condition=False, factorial_conditions=['cue_condition', 'task_condition'], split_by_block_num=False, is_go_no_go=False):\n",
    "    \"\"\"\n",
    "    Calculates the omission rate for given test trials based on specific conditions.\n",
    "    \n",
    "    Omission rate refers to the proportion of missing reaction times (RTs) in the data. This function\n",
    "    supports calculations for both standard and factorial conditions. Results can optionally be split \n",
    "    by block number.\n",
    "    \n",
    "    Input:\n",
    "       df: DataFrame containing task data for a specific task for a single subject.\n",
    "      test_trial: Name of the column indicating the type of trial. Default is 'test_trial'.\n",
    "      condition_col: Name of the column representing the condition. Default is 'condition'.\n",
    "      factorial_condition: Boolean to specify if the data has factorial conditions. Default is False.\n",
    "      factorial_conditions: List of columns indicating factorial conditions. Default is ['cue_condition', 'task_condition'].\n",
    "      split_by_block_num: Boolean to specify if results should be split by block number. Default is False.\n",
    "    \n",
    "    Output:\n",
    "      Prints the omission rate for the specified conditions.\n",
    "    \"\"\"\n",
    "     \n",
    "    test_trials__df = df[df['trial_id'] == test_trial]\n",
    "    \n",
    "    if factorial_condition:\n",
    "        grouping_columns = factorial_conditions\n",
    "    else:\n",
    "        grouping_columns = [condition_col]\n",
    "    \n",
    "    if split_by_block_num:\n",
    "        grouping_columns.append('block_num')\n",
    "    \n",
    "    omission_rate = test_trials__df.groupby(grouping_columns).apply(lambda group: group['rt'].isna().mean())\n",
    "\n",
    "    if is_go_no_go:\n",
    "        omission_rate = omission_rate['go']\n",
    "\n",
    "    print(omission_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_omission_rate__span(df):\n",
    "    \"\"\"\n",
    "    Calculates the omission rate for the 'span' task based on specific trial types and response lengths.\n",
    "    \n",
    "    This function targets the 'span' task data to calculate two types of omissions: \n",
    "    1) Completely empty responses, and \n",
    "    2) Incomplete responses (i.e., responses with a length between 1 and 3).\n",
    "    \n",
    "    Additionally, this function calculates the omission rate for 'test_inter-stimulus' trials, \n",
    "    which is the proportion of missing reaction times (RTs) in these trials.\n",
    "\n",
    "    Input:\n",
    "       df: DataFrame containing task data for a specific task for a single subject.\n",
    "    \n",
    "    Output:\n",
    "      Prints the mean number of empty responses, the mean number of incomplete responses, \n",
    "      and the omission rate for 'test_inter-stimulus' trials.\n",
    "    \"\"\"\n",
    "    test_response_trials__df = df[df['trial_id'] == 'test_response'].copy()\n",
    "    test_processing_trials__df = df[df['trial_id'] == 'test_inter-stimulus']\n",
    "\n",
    "    # Convert the strings in the 'response' column to actual lists\n",
    "    test_response_trials__df['response'] = test_response_trials__df['response'].apply(ast.literal_eval)\n",
    "\n",
    "    omission_rate_processing_trials = test_processing_trials__df['rt'].isna().mean()\n",
    "\n",
    "    # Calculate the number of empty and incomplete responses\n",
    "    test_response_trials__df['empty'] = test_response_trials__df['response'].apply(lambda x: len(x) == 0)\n",
    "    test_response_trials__df['incomplete'] = test_response_trials__df['response'].apply(lambda x: 0 < len(x) < 4)\n",
    "\n",
    "    # Get the mean of each type\n",
    "    mean_empty = test_response_trials__df['empty'].mean()\n",
    "    mean_incomplete = test_response_trials__df['incomplete'].mean()\n",
    "\n",
    "    print(f\"Mean number of empty responses: {mean_empty}\")\n",
    "    print(f\"Mean number of incomplete responses: {mean_incomplete}\")\n",
    "\n",
    "    print(f\"Omission rate processing trials: {omission_rate_processing_trials}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_accuracy(df, correct_trial_col='correct_trial', condition_col='condition', test_trial='test_trial', factorial_condition=False, factorial_conditions=[], split_by_block_num=False):\n",
    "    \"\"\"\n",
    "    Calculates the average accuracy for given test trials based on specified conditions.\n",
    "    \n",
    "    This function computes the mean accuracy for a given set of test trials. It allows for grouping\n",
    "    by a single condition or multiple factorial conditions. The option to further split by block number\n",
    "    is also available. The accuracy is determined by averaging the values in the `correct_trial_col`.\n",
    "    \n",
    "    Input:\n",
    "      df: DataFrame containing task data for a specific task for a single subject.\n",
    "      correct_trial_col (optional): Name of the column indicating correct trials (default is 'correct_trial').\n",
    "      condition_col (optional): Name of the main condition column for grouping (default is 'condition').\n",
    "      test_trial (optional): Specifies the trial type to be considered for accuracy calculation (default is 'test_trial').\n",
    "      factorial_condition (optional): Boolean flag indicating if factorial conditions should be used for grouping (default is False).\n",
    "      factorial_conditions (optional): List of columns to be used for factorial grouping (default is an empty list).\n",
    "      split_by_block_num (optional): Boolean flag to determine if results should be split by block number (default is False).\n",
    "      \n",
    "    Output:\n",
    "      Prints the average accuracy grouped by the specified conditions.\n",
    "    \"\"\"\n",
    "   \n",
    "    test_trials__df = df[df['trial_id'] == test_trial]\n",
    "    \n",
    "    if factorial_condition:\n",
    "        grouping_columns = factorial_conditions\n",
    "    else:\n",
    "        grouping_columns = [condition_col]\n",
    "    \n",
    "    if split_by_block_num:\n",
    "        grouping_columns.append('block_num')\n",
    "    \n",
    "    accuracy_by_condition = test_trials__df.groupby(grouping_columns)[correct_trial_col].mean()\n",
    "    \n",
    "    print(accuracy_by_condition)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing sample data files for different RDoC tasks.\n",
    "ax_cpt__df = pd.read_csv('./data/ax_cpt_rdoc_23-10-25-16:51.json.csv') # need to use probe trial\n",
    "cued_ts__df = pd.read_csv('./data/cued_task_switching_rdoc_23-10-25-17:36.json.csv') # need to use factorial conditions, cue_condition & task_condition\n",
    "flanker__df = pd.read_csv('./data/flanker_rdoc_23-10-25-18:02.json.csv')\n",
    "go_nogo__df = pd.read_csv('./data/go_nogo_rdoc_23-10-25-18:20.json.csv')\n",
    "n_back__df = pd.read_csv('./data/n_back_rdoc_23-10-25-18:33.json.csv') # need to use delay instead\n",
    "span__df = pd.read_csv('./data/span_rdoc__behavioral_23-10-25-21:45.json.csv') \n",
    "spatial_ts__df = pd.read_csv('./data/spatial_task_switching_rdoc_23-10-25-20:33.json.csv')\n",
    "spatial_cueing__df = pd.read_csv('./data/spatial_cueing_rdoc_23-10-25-20:56.json.csv')\n",
    "stroop__df = pd.read_csv('./data/stroop_rdoc_23-10-25-18:44.json.csv')\n",
    "stop_signal__df = pd.read_csv('./data/stop_signal_rdoc_23-10-25-19:22.json.csv')\n",
    "visual_search__df = pd.read_csv('./data/visual_search_rdoc_23-10-25-19:10.json.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "condition\n",
      "congruent      0.616667\n",
      "incongruent    0.516667\n",
      "Name: correct_trial, dtype: float64\n",
      "condition\n",
      "congruent      579.702703\n",
      "incongruent    611.483871\n",
      "dtype: float64\n",
      "condition\n",
      "congruent      0.350000\n",
      "incongruent    0.416667\n",
      "dtype: float64\n",
      "condition\n",
      "go      0.685185\n",
      "nogo    0.888889\n",
      "Name: correct_trial, dtype: float64\n",
      "condition\n",
      "go      360.891892\n",
      "nogo           NaN\n",
      "dtype: float64\n",
      "condition\n",
      "doublecue    0.361111\n",
      "invalid      0.388889\n",
      "nocue        0.319444\n",
      "valid        0.425926\n",
      "Name: correct_trial, dtype: float64\n",
      "condition\n",
      "doublecue    384.538462\n",
      "invalid      432.428571\n",
      "nocue        408.000000\n",
      "valid        376.869565\n",
      "dtype: float64\n",
      "condition\n",
      "doublecue    0.611111\n",
      "invalid      0.611111\n",
      "nocue        0.680556\n",
      "valid        0.574074\n",
      "dtype: float64\n",
      "condition\n",
      "congruent      0.390244\n",
      "incongruent    0.379747\n",
      "Name: correct_trial, dtype: float64\n",
      "condition\n",
      "congruent      544.0\n",
      "incongruent    651.1\n",
      "dtype: float64\n",
      "condition\n",
      "congruent      0.512195\n",
      "incongruent    0.569620\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# these are the ones that can be called normally, without customizing any arguments\n",
    "calculate_average_accuracy(flanker__df)\n",
    "calculate_average_rt(flanker__df)\n",
    "calculate_omission_rate(flanker__df)\n",
    "\n",
    "calculate_average_accuracy(go_nogo__df)\n",
    "calculate_average_rt(go_nogo__df)\n",
    "\n",
    "calculate_average_accuracy(spatial_cueing__df)\n",
    "calculate_average_rt(spatial_cueing__df)\n",
    "calculate_omission_rate(spatial_cueing__df)\n",
    "\n",
    "\n",
    "calculate_average_accuracy(stroop__df)\n",
    "calculate_average_rt(stroop__df)\n",
    "calculate_omission_rate(stroop__df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "condition\n",
      "AX    0.333333\n",
      "AY    0.400000\n",
      "BX    0.533333\n",
      "BY    0.400000\n",
      "Name: correct_trial, dtype: float64\n",
      "condition\n",
      "AX    554.550000\n",
      "AY    542.916667\n",
      "BX    462.312500\n",
      "BY    486.583333\n",
      "dtype: float64\n",
      "condition\n",
      "AX    0.533333\n",
      "AY    0.566667\n",
      "BX    0.366667\n",
      "BY    0.566667\n",
      "dtype: float64\n",
      "cue_condition  task_condition\n",
      "na             na                1.000000\n",
      "stay           stay              0.708333\n",
      "switch         stay              0.541667\n",
      "               switch            0.604167\n",
      "Name: correct_trial, dtype: float64\n",
      "cue_condition  task_condition\n",
      "na             na                563.666667\n",
      "stay           stay              682.823529\n",
      "switch         stay              706.923077\n",
      "               switch            664.068966\n",
      "dtype: float64\n",
      "cue_condition  task_condition\n",
      "na             na                0.000000\n",
      "stay           stay              0.208333\n",
      "switch         stay              0.312500\n",
      "               switch            0.270833\n",
      "dtype: float64\n",
      "0.3148148148148148\n",
      "delay\n",
      "1.0    0.861538\n",
      "2.0    0.542857\n",
      "Name: correct_trial, dtype: float64\n",
      "delay\n",
      "1.0    692.339286\n",
      "2.0    585.605263\n",
      "dtype: float64\n",
      "delay\n",
      "1.0    0.030769\n",
      "2.0    0.200000\n",
      "dtype: float64\n",
      "condition\n",
      "same-domain     0.705882\n",
      "storage-only    0.583333\n",
      "Name: correct_trial, dtype: float64\n",
      "condition\n",
      "same-domain    752.25974\n",
      "dtype: float64\n",
      "Mean number of empty responses: 0.3333333333333333\n",
      "Mean number of incomplete responses: 0.0625\n",
      "Omission rate processing trials: 0.3567708333333333\n",
      "stop_acc: 0.6363636363636364\n",
      "go_acc: 0.4583333333333333\n",
      "avg_go_rt: 549.0545454545454\n",
      "max_SSD: 1000.0\n",
      "min_SSD: 200.0\n",
      "mean_SSD: 609.4444444444445\n",
      "stop_success: 0.6363636363636364\n",
      "stop_fail: 0.36363636363636365\n",
      "go_success: 0.4583333333333333\n",
      "stop_omission_rate: 0.6666666666666666\n",
      "go_omission_rate: 0.5083333333333333\n",
      "condition    num_stimuli\n",
      "conjunction  8.0            0.960000\n",
      "             24.0           0.928571\n",
      "feature      8.0            1.000000\n",
      "             24.0           1.000000\n",
      "Name: correct_trial, dtype: float64\n",
      "condition    num_stimuli\n",
      "conjunction  8.0            716.208333\n",
      "             24.0           964.807692\n",
      "feature      8.0            674.320000\n",
      "             24.0           589.227273\n",
      "dtype: float64\n",
      "condition    num_stimuli\n",
      "conjunction  8.0            0.528302\n",
      "             24.0           0.377778\n",
      "feature      8.0            0.456522\n",
      "             24.0           0.541667\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# below should be the only instances where the function call is something other than the default \n",
    "# most tasks share the same structure (e.g condition column called condition, test trials called test_trial, but the ones below do not)\n",
    "# note: attention checks is always the same\n",
    "\n",
    "### ax-cpt\n",
    "calculate_average_accuracy(ax_cpt__df, test_trial='test_probe')  # need different test_trial than test_trial, must be test_probe instead\n",
    "calculate_average_rt(ax_cpt__df, test_trial='test_probe')\n",
    "calculate_omission_rate(ax_cpt__df, test_trial='test_probe')\n",
    "\n",
    "### cued ts\n",
    "calculate_average_accuracy(cued_ts__df, factorial_condition=True, factorial_conditions=['cue_condition', 'task_condition']) # need to use factorial for cue_condition and task_condition since separate cols\n",
    "calculate_average_rt(cued_ts__df, factorial_condition=True, factorial_conditions=['cue_condition', 'task_condition'])\n",
    "calculate_omission_rate(cued_ts__df, factorial_condition=True, factorial_conditions=['cue_condition', 'task_condition'])\n",
    "### Note: Looks like we don't have to do the above for spatial_ts since it already combines in the conditions (e.g. tstay_cstay) prior to exporting data. \n",
    "\n",
    "### gonogo\n",
    "calculate_omission_rate(go_nogo__df, is_go_no_go=True) # since omission rate is only for go trial\n",
    "\n",
    "### nback  \n",
    "calculate_average_accuracy(n_back__df, condition_col='delay') # need to use delay instead of 'match' , 'mismatch' condition \n",
    "calculate_average_rt(n_back__df, condition_col='delay') # need to use delay instead of 'match' , 'mismatch' condition \n",
    "calculate_omission_rate(n_back__df, condition_col='delay')\n",
    "\n",
    "### span (simple & operation; i.e. storage-only and same-domain)\n",
    "calculate_average_accuracy(span__df, test_trial='test_response')\n",
    "calculate_average_rt(span__df, test_trial='test_inter-stimulus', correct_trial_col='correct_response')\n",
    "calculate_omission_rate__span(span__df) # need something different for omissions since no response in test_response is [] and incomplete is [].length < 4\n",
    "\n",
    "### stop signal\n",
    "get_stopping_data(stop_signal__df)\n",
    "\n",
    "### visual search\n",
    "calculate_average_accuracy(visual_search__df, factorial_condition=True, factorial_conditions=['condition', 'num_stimuli']) # need to use factorial for load and feature/conjunction\n",
    "calculate_average_rt(visual_search__df, factorial_condition=True, factorial_conditions=['condition', 'num_stimuli'])\n",
    "calculate_omission_rate(visual_search__df, factorial_condition=True, factorial_conditions=['condition', 'num_stimuli'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note about modifying function calls\n",
    "These are tasks that need slightly different arguments for accuracy, rt, and/or omission calculations:\n",
    "\n",
    "### ax-cpt\n",
    "```python\n",
    "calculate_average_accuracy(ax_cpt__df, test_trial='test_probe')  # need different test_trial than test_trial, must be test_probe instead\n",
    "```\n",
    "\n",
    "### cued ts\n",
    "```python \n",
    "calculate_average_accuracy(cued_ts__df, factorial_condition=True, factorial_conditions=['cue_condition', 'task_condition']) # need to use factorial for cue_condition and task_condition since separate cols\n",
    "```\n",
    "#### Note: Looks like we don't have to do this for spatial_ts since it already combines in the conditions (e.g. tstay_cstay) prior to exporting data. \n",
    "\n",
    "### nback  \n",
    "```python\n",
    "calculate_average_accuracy(n_back__df, condition_col='delay') # need to use delay instead of 'match' , 'mismatch' condition \n",
    "calculate_average_rt(n_back__df, condition_col='delay') # need to use delay instead of 'match' , 'mismatch' condition \n",
    "```\n",
    "\n",
    "### span (simple & operation; i.e. storage-only and same-domain)\n",
    "```python\n",
    "calculate_average_accuracy(span__df, test_trial='test_response')\n",
    "calculate_average_rt(span__df, test_trial='test_inter-stimulus', correct_trial_col='correct_response')\n",
    "calculate_omission_rate__span(span__df) # need something different for omissions since no response in test_response is [] and incomplete is [].length < 4\n",
    "```\n",
    "\n",
    "### stop signal\n",
    "```python\n",
    "get_stopping_data(stop_signal__df)\n",
    "```\n",
    "\n",
    "### visual search\n",
    "```python\n",
    "calculate_average_accuracy(visual_search__df, factorial_condition=True, factorial_conditions=['condition', 'num_stimuli']) # need to use factorial for load and feature/conjunction\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
