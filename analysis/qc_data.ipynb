{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import ast\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## thresholds \n",
    "attention_check_accuracy__threshold = 0.5\n",
    "\n",
    "mean_accuracy__thresholds = {\n",
    "    'ax_cpt': {'AX': 0.9, 'BX': 0.75, 'AY': 0.75, 'BY': 0.75},\n",
    "    'cued_ts': {('switch', 'switch'): .85, ('switch', 'stay'): .9, ('stay', 'stay'): .9},\n",
    "    'flanker': {'congruent': 0.9, 'incongruent': 0.75},\n",
    "    'go_nogo': {'go': 0.9, 'nogo': 0.7},\n",
    "    'n_back': {1.0: 0.85, 2.0: 0.7},\n",
    "    'span': {'response': .5, 'processing': .5},\n",
    "    'spatial_cueing': {'valid': 0.85, 'invalid': 0.75, 'nocue': 0.8, 'doublecue': 0.8},\n",
    "    'spatial_ts': {'tswitch_cswitch': 0.85, 'tstay_cstay': 0.9, 'tstay_cswitch': .9},\n",
    "    'stop_signal': 0.5,\n",
    "    'stroop': {'congruent': 0.9, 'incongruent': 0.7},\n",
    "    'visual_search':{\n",
    "    ('conjunction', 8.0): 0.95,\n",
    "    ('conjunction', 24.0): 0.85,\n",
    "    ('feature', 8.0): 0.99,\n",
    "    ('feature', 24.0): 0.95\n",
    "    }\n",
    "}\n",
    "\n",
    "mean_rt__thresholds = {\n",
    "    'ax_cpt': {'AX': 750, 'BX': 900, 'AY': 850, 'BY': 900},\n",
    "    'cued_ts': {('switch', 'switch'): 900, ('switch', 'stay'): 700, ('stay', 'stay'): 700},\n",
    "    'flanker': {'congruent': 600, 'incongruent': 800},\n",
    "    'go_nogo': {'go': 700},\n",
    "    'n_back': {1.0: 850, 2.0: 950},\n",
    "    'span': {'response': 1000, 'processing': 1000},\n",
    "    'spatial_cueing': {'valid': 650, 'invalid': 750, 'nocue': 700, 'doublecue': 700},\n",
    "    'spatial_ts': {'tswitch_cswitch': 900, 'tstay_cstay': 700, 'tstay_cswitch': 700},\n",
    "    'stop_signal': 650,\n",
    "    'stroop': {'congruent': 650, 'incongruent': 900},\n",
    "    'visual_search': {\n",
    "    ('conjunction', 8.0): 1000,\n",
    "    ('conjunction', 24.0): 1200,\n",
    "    ('feature', 8.0): 600,\n",
    "    ('feature', 24.0): 800\n",
    "}\n",
    "}\n",
    "\n",
    "omission_rate__thresholds = {\n",
    "    'ax_cpt': .1,\n",
    "    'cued_ts': {('switch', 'switch'): .1, ('switch', 'stay'): .1, ('stay', 'stay'): .1},\n",
    "    'flanker': 0.1,\n",
    "    'go_nogo': {'go': 0.1},\n",
    "    'n_back': {1.0: 0.1, 2.0: 0.15},\n",
    "    'span': {'response': .2, 'processing': .2},\n",
    "    'spatial_cueing': {'valid': 0.1, 'invalid': 0.1, 'nocue': 0.1, 'doublecue': 0.1},\n",
    "    'spatial_ts': {'tswitch_cswitch': .1, 'tstay_cstay': .1, 'tstay_cswitch': .1},\n",
    "    'stop_signal': 0.1,\n",
    "    'stroop': 0.1,\n",
    "    'visual_search': {\n",
    "    ('conjunction', 8.0): 0.1,\n",
    "    ('conjunction', 24.0): 0.15,\n",
    "    ('feature', 8.0): 0.05,\n",
    "    ('feature', 24.0): 0.1\n",
    "}\n",
    "}\n",
    "\n",
    "# Define the stopping task thresholds\n",
    "mean_stopping__thresholds = {\n",
    "    'stop_signal': {\n",
    "        'stop_acc': 0.5,  \n",
    "        'go_acc': 0.5,    \n",
    "        'avg_go_rt': 600,\n",
    "        'stop_success': 0.5,  \n",
    "        'stop_fail': 0.5,    \n",
    "        'go_success': 0.5,   \n",
    "        'stop_omission_rate': 0.2,  \n",
    "        'go_omission_rate': 0.2    \n",
    "    }\n",
    "}\n",
    "\n",
    "# Define the 'span' task thresholds outside of the function\n",
    "mean_span__thresholds = {\n",
    "    'accuracy': {\n",
    "        'same-domain': 0.7, \n",
    "        'storage-only': 0.6, \n",
    "    },\n",
    "    'rt': {\n",
    "        'same-domain': 700, \n",
    "    },\n",
    "    'omission': {\n",
    "        'omission_rate_empty_response_trials': 0.2,  \n",
    "        'omission_rate_incomplete_response_trials': 0.1,\n",
    "        'omission_rate_processing_trials': 0.3, \n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedback_generator(task, accuracies=None, rts=None, omissions=None, attention_check_accuracy=None, factorial_condition=False, stopping_data=None):\n",
    "    feedbacks = []\n",
    "\n",
    "    def get_threshold(thresholds, condition, factorial_condition):\n",
    "        if factorial_condition and isinstance(thresholds, dict):\n",
    "            # For factorial conditions, we expect a dictionary with tuple keys\n",
    "            return thresholds.get(condition)\n",
    "        elif not factorial_condition and isinstance(thresholds, dict):\n",
    "            # For non-factorial conditions, we extract the value using the condition as the key\n",
    "            return thresholds.get(condition)\n",
    "        else:\n",
    "            # When thresholds is not a dictionary, it means it's a direct value applicable to all conditions\n",
    "            return thresholds\n",
    "\n",
    "    if accuracies:\n",
    "        if task not in mean_accuracy__thresholds:\n",
    "            return [\"Invalid task name.\"]\n",
    "        for condition, value in accuracies.items():\n",
    "            threshold = mean_accuracy__thresholds[task]\n",
    "            target = get_threshold(threshold, condition, factorial_condition)\n",
    "            if target is None:\n",
    "                # feedbacks.append(f\"Invalid condition {condition} for the task {task}.\")\n",
    "                continue\n",
    "            if value < target:\n",
    "                condition_label = ' and '.join(condition) if factorial_condition else condition\n",
    "                feedback = f\"Your accuracy of {value*100:.2f}% is low for the {condition_label} condition of {task}.\"\n",
    "                feedbacks.append(feedback)\n",
    "\n",
    "    if omissions:\n",
    "        if task not in omission_rate__thresholds:\n",
    "            return [\"Invalid task name.\"]\n",
    "        for condition, value in omissions.items():\n",
    "            threshold = omission_rate__thresholds[task]\n",
    "            target = get_threshold(threshold, condition, factorial_condition)\n",
    "            if target is None:\n",
    "                # feedbacks.append(f\"Invalid condition {condition} for the task {task}.\")\n",
    "                continue\n",
    "            if value > target:\n",
    "                condition_label = ' and '.join(condition) if factorial_condition else condition\n",
    "                feedback = f\"Your omission rate of {value*100:.2f}% is high for the {condition_label} condition of {task}.\"\n",
    "                feedbacks.append(feedback)\n",
    "\n",
    "    if rts:\n",
    "        if task not in mean_rt__thresholds:\n",
    "            return [\"Invalid task name.\"]\n",
    "        for condition, value in rts.items():\n",
    "            threshold = mean_rt__thresholds[task]\n",
    "            target = get_threshold(threshold, condition, factorial_condition)\n",
    "            if target is None:\n",
    "                # feedbacks.append(f\"Invalid condition {condition} for the task {task}.\")\n",
    "                continue\n",
    "            if value > target:\n",
    "                condition_label = ' and '.join(condition) if factorial_condition else condition\n",
    "                feedback = f\"Your average RT of {value:.2f}ms is high for the {condition_label} condition of {task}.\"\n",
    "                feedbacks.append(feedback)\n",
    "\n",
    "    if attention_check_accuracy is not None:\n",
    "        if attention_check_accuracy < attention_check_accuracy__threshold:\n",
    "            feedback = f\"Your attention check accuracy of {attention_check_accuracy*100:.2f}% is below the required threshold of {attention_check_accuracy__threshold*100:.2f}%. Please ensure you are remaining attentive throughout the tasks.\"\n",
    "            feedbacks.append(feedback)\n",
    "\n",
    "    if stopping_data and task == 'stop_signal':\n",
    "        stopping_thresholds = mean_stopping__thresholds[task]\n",
    "        for metric, value in stopping_data.items():\n",
    "            threshold = stopping_thresholds.get(metric)\n",
    "            if threshold is not None:\n",
    "                if 'acc' in metric and value < threshold:\n",
    "                    feedback = f\"Your {metric.replace('_', ' ')} of {value*100:.2f}% is below the threshold of {threshold*100:.2f}%.\"\n",
    "                    feedbacks.append(feedback)\n",
    "                elif 'rt' in metric and value > threshold:\n",
    "                    feedback = f\"Your {metric.replace('_', ' ')} of {value:.2f}ms is above the threshold of {threshold:.2f}ms.\"\n",
    "                    feedbacks.append(feedback)\n",
    "                elif 'omission_rate' in metric and value > threshold:\n",
    "                    feedback = f\"Your {metric.replace('_', ' ')} of {value*100:.2f}% is above the threshold of {threshold*100:.2f}%.\"\n",
    "                    feedbacks.append(feedback)\n",
    "\n",
    "    # Section to handle 'span' task feedback\n",
    "    if task == 'span':\n",
    "        span_accuracy_thresholds = mean_span__thresholds['accuracy']\n",
    "        span_rt_thresholds = mean_span__thresholds['rt']\n",
    "        span_omission_thresholds = mean_span__thresholds['omission']\n",
    "        \n",
    "        # Process accuracies for 'span'\n",
    "        if accuracies:\n",
    "            for condition, value in accuracies.items():\n",
    "                threshold = span_accuracy_thresholds.get(condition)\n",
    "                if threshold is None:\n",
    "                    # feedbacks.append(f\"Invalid condition {condition} for the task {task}.\")\n",
    "                    continue\n",
    "                if value < threshold:\n",
    "                    feedback = f\"Your accuracy for {condition} is low at {value*100:.2f}%, below the threshold of {threshold*100:.2f}%.\"\n",
    "                    feedbacks.append(feedback)\n",
    "\n",
    "        # Process RTs for 'span'\n",
    "        if rts:\n",
    "            for condition, value in rts.items():\n",
    "                threshold = span_rt_thresholds.get(condition)\n",
    "                if threshold is None:\n",
    "                    # feedbacks.append(f\"Invalid condition {condition} for the task {task}.\")\n",
    "                    continue\n",
    "                if value > threshold:\n",
    "                    feedback = f\"Your reaction time for {condition} is high at {value:.2f}ms, above the threshold of {threshold:.2f}ms.\"\n",
    "                    feedbacks.append(feedback)\n",
    "\n",
    "        # Process omissions for 'span'\n",
    "        if omissions:\n",
    "            for metric, value in omissions.items():\n",
    "                threshold = span_omission_thresholds.get(metric)\n",
    "                if threshold is None:\n",
    "                    feedbacks.append(f\"Invalid metric {metric} for the task {task}.\")\n",
    "                    continue\n",
    "                if value > threshold:\n",
    "                    feedback = f\"Your {metric.replace('_', ' ')} is high at {value*100:.2f}%, above the threshold of {threshold*100:.2f}%.\"\n",
    "                    feedbacks.append(feedback)\n",
    "\n",
    "    return feedbacks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stopping_data(df, split_by_block_num=False):\n",
    "    \"\"\"\n",
    "    Extracts and calculates metrics related to 'stop' and 'go' conditions for test trials.\n",
    "    \n",
    "    The function processes data to compute key metrics, including accuracy, response time (rt),\n",
    "    stop signal delay (SSD), and omission rates, for both 'stop' and 'go' trial conditions.\n",
    "    The results can be grouped by block numbers if required.\n",
    "    \n",
    "    Input:\n",
    "      df: DataFrame containing task data for a specific task for a single subject.\n",
    "      split_by_block_num (optional): Boolean flag to determine if results should be grouped \n",
    "      by block number (default is False).\n",
    "      \n",
    "    Output:\n",
    "      Prints the computed metrics either grouped by block numbers or in an aggregated form.\n",
    "      \n",
    "    Metrics Calculated:\n",
    "      - stop_acc: Mean accuracy for 'stop' trials.\n",
    "      - go_acc: Mean accuracy for 'go' trials.\n",
    "      - avg_go_rt: Average response time for correct 'go' trials.\n",
    "      - max_SSD: Maximum stop signal delay.\n",
    "      - min_SSD: Minimum stop signal delay.\n",
    "      - mean_SSD: Average stop signal delay.\n",
    "      - stop_success: Percentage of successful stops.\n",
    "      - stop_fail: Percentage of failed stops.\n",
    "      - go_success: Percentage of successful 'go' trials.\n",
    "      - stop_omission_rate: Omission rate for 'stop' trials.\n",
    "      - go_omission_rate: Omission rate for 'go' trials.\n",
    "    \"\"\"\n",
    "    test_trials__df = df[(df['trial_id'] == 'test_trial')]\n",
    "    \n",
    "    grouping_column = 'block_num' if split_by_block_num else None\n",
    "\n",
    "    # If we're splitting by block_num, group the data by block_num\n",
    "    if split_by_block_num:\n",
    "        stop_trials = test_trials__df[(test_trials__df['condition'] == 'stop')].groupby(grouping_column)\n",
    "        go_trials = test_trials__df[(test_trials__df['condition'] == 'go')].groupby(grouping_column)\n",
    "    else:\n",
    "        stop_trials = test_trials__df[(test_trials__df['condition'] == 'stop')]\n",
    "        go_trials = test_trials__df[(test_trials__df['condition'] == 'go')]\n",
    "\n",
    "    # Define a helper function to calculate metrics for a given group\n",
    "    def calculate_metrics(group):\n",
    "        stop_acc = group[group['condition'] == 'stop']['stop_acc'].mean()\n",
    "        go_acc = group[group['condition'] == 'go']['go_acc'].mean()\n",
    "\n",
    "        go_correct_trials = group[(group['condition'] == 'go') & (group['go_acc'] == 1)]\n",
    "        avg_go_rt = go_correct_trials['rt'].mean()\n",
    "\n",
    "        max_SSD = group['SSD'].max()\n",
    "        min_SSD = group['SSD'].min()\n",
    "        mean_SSD = group['SSD'].mean()\n",
    "\n",
    "        stop_success = group[group['condition'] == 'stop']['stop_acc'].mean()\n",
    "        stop_fail = 1 - stop_success\n",
    "\n",
    "        go_success = group[group['condition'] == 'go']['go_acc'].mean()\n",
    "        go_omission_rate = group[group['condition'] == 'go']['rt'].isna().mean()\n",
    "\n",
    "        return {\n",
    "            \"stop_acc\": stop_acc,\n",
    "            \"go_acc\": go_acc,\n",
    "            \"avg_go_rt\": avg_go_rt,\n",
    "            \"max_SSD\": max_SSD,\n",
    "            \"min_SSD\": min_SSD,\n",
    "            \"mean_SSD\": mean_SSD,\n",
    "            \"stop_success\": stop_success,\n",
    "            \"stop_fail\": stop_fail,\n",
    "            \"go_success\": go_success,\n",
    "            \"go_omission_rate\": go_omission_rate\n",
    "        }\n",
    "\n",
    "    # If we're splitting by block_num, apply the helper function to each group\n",
    "    if split_by_block_num:\n",
    "        results = test_trials__df.groupby(grouping_column).apply(calculate_metrics)\n",
    "    else:\n",
    "        results = calculate_metrics(test_trials__df)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_attention_check_accuracy(df):\n",
    "    \"\"\"\n",
    "    Calculates the attention check accuracy for attention checks\n",
    "    \n",
    "    This function computes the accuracy for a given set of attention checks for a single task df.\n",
    "    \n",
    "    Input:\n",
    "      df: DataFrame containing task data for a specific task for a single subject.  \n",
    "          \n",
    "    Output:\n",
    "      Prints the overall attention check accuracy for a given task df for a single subject. \n",
    "    \"\"\"\n",
    "\n",
    "    test_trials__df = df[(df['trial_id'] == 'test_attention_check')]\n",
    "    attention_check_accuracy = test_trials__df['correct_trial'].mean()\n",
    "    return attention_check_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_rt(df, condition_col='condition', test_trial='test_trial', correct_trial_col='correct_trial', factorial_condition=False,factorial_conditions=[], split_by_block_num=False):\n",
    "    \"\"\"\n",
    "    Calculates the average reaction time (RT) for given test trials based on specific conditions.\n",
    "    \n",
    "    This function can handle both standard conditions and factorial conditions. Additionally,\n",
    "    results can optionally be split by block number.\n",
    "    \n",
    "    Input:\n",
    "      df: DataFrame containing the task data.\n",
    "      condition_col: Name of the column representing the condition. Default is 'condition'.\n",
    "      test_trial: Name of the column indicating the type of trial. Default is 'test_trial'.\n",
    "      correct_trial_col: Column indicating if the trial was correctly executed. Default is 'correct_trial'.\n",
    "      factorial_condition: Boolean to specify if the data has factorial conditions. Default is False.\n",
    "      factorial_conditions: List of columns indicating factorial conditions. Default is [].\n",
    "      split_by_block_num: Boolean to specify if results should be split by block number. Default is False.\n",
    "    \n",
    "    Output:\n",
    "      Prints the average RT for the specified conditions.\n",
    "    \"\"\"    \n",
    "    test_trials__df = df[(df['trial_id'] == test_trial) & (df[correct_trial_col] == 1)]\n",
    "    \n",
    "    if factorial_condition:\n",
    "        grouping_columns = factorial_conditions\n",
    "    else:\n",
    "        grouping_columns = [condition_col]\n",
    "    \n",
    "    if split_by_block_num:\n",
    "        grouping_columns.append('block_num')\n",
    "    \n",
    "    rt_by_condition = test_trials__df.groupby(grouping_columns).apply(lambda group: group['rt'].mean())\n",
    "    \n",
    "    return rt_by_condition.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_omission_rate(df, test_trial='test_trial', condition_col='condition', factorial_condition=False, factorial_conditions=[], split_by_block_num=False, is_go_no_go=False):\n",
    "    \"\"\"\n",
    "    Calculates the omission rate for given test trials based on specific conditions.\n",
    "    \n",
    "    Omission rate refers to the proportion of missing reaction times (RTs) in the data. This function\n",
    "    supports calculations for both standard and factorial conditions. Results can optionally be split \n",
    "    by block number.\n",
    "    \n",
    "    Input:\n",
    "       df: DataFrame containing task data for a specific task for a single subject.\n",
    "      test_trial: Name of the column indicating the type of trial. Default is 'test_trial'.\n",
    "      condition_col: Name of the column representing the condition. Default is 'condition'.\n",
    "      factorial_condition: Boolean to specify if the data has factorial conditions. Default is False.\n",
    "      factorial_conditions: List of columns indicating factorial conditions. Default is [].\n",
    "      split_by_block_num: Boolean to specify if results should be split by block number. Default is False.\n",
    "    \n",
    "    Output:\n",
    "      Prints the omission rate for the specified conditions.\n",
    "    \"\"\"\n",
    "     \n",
    "    test_trials__df = df[df['trial_id'] == test_trial]\n",
    "    \n",
    "    if factorial_condition:\n",
    "        grouping_columns = factorial_conditions\n",
    "    else:\n",
    "        grouping_columns = [condition_col]\n",
    "    \n",
    "    if split_by_block_num:\n",
    "        grouping_columns.append('block_num')\n",
    "    \n",
    "    omission_rate = test_trials__df.groupby(grouping_columns).apply(lambda group: group['rt'].isna().mean())\n",
    "\n",
    "    if is_go_no_go:\n",
    "        omission_rate['nogo'] = None\n",
    "\n",
    "    return omission_rate.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_omission_rate__span(df):\n",
    "    \"\"\"\n",
    "    Calculates the omission rate for the 'span' task based on specific trial types and response lengths.\n",
    "    \n",
    "    This function targets the 'span' task data to calculate two types of omissions: \n",
    "    1) Completely empty responses, and \n",
    "    2) Incomplete responses (i.e., responses with a length between 1 and 3).\n",
    "    \n",
    "    Additionally, this function calculates the omission rate for 'test_inter-stimulus' trials, \n",
    "    which is the proportion of missing reaction times (RTs) in these trials.\n",
    "\n",
    "    Input:\n",
    "       df: DataFrame containing task data for a specific task for a single subject.\n",
    "    \n",
    "    Output:\n",
    "      Prints the mean number of empty responses, the mean number of incomplete responses, \n",
    "      and the omission rate for 'test_inter-stimulus' trials.\n",
    "    \"\"\"\n",
    "   \n",
    "    test_response_trials__df = df[df['trial_id'] == 'test_response'].copy()\n",
    "    test_processing_trials__df = df[df['trial_id'] == 'test_inter-stimulus'].copy()\n",
    "\n",
    "    # Convert the strings in the 'response' column to actual lists\n",
    "    test_response_trials__df['response'] = test_response_trials__df['response']\n",
    "\n",
    "    omission_rate_processing_trials = test_processing_trials__df['rt'].isna().mean()\n",
    "\n",
    "    # Calculate the number of empty and incomplete responses\n",
    "    test_response_trials__df['empty'] = test_response_trials__df['response'].apply(lambda x: len(x) == 0)\n",
    "    test_response_trials__df['incomplete'] = test_response_trials__df['response'].apply(lambda x: 0 < len(x) < 4)\n",
    "\n",
    "    # Get the mean of each type\n",
    "    mean_empty = test_response_trials__df['empty'].mean()\n",
    "    mean_incomplete = test_response_trials__df['incomplete'].mean()\n",
    "\n",
    "    # Return the results as a dictionary\n",
    "    return {\n",
    "        'omission_rate_empty_response_trials': mean_empty,\n",
    "        'omission_rate_incomplete_response_trials': mean_incomplete,\n",
    "        'omission_rate_processing_trials': omission_rate_processing_trials\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_accuracy(df, correct_trial_col='correct_trial', condition_col='condition', test_trial='test_trial', factorial_condition=False, factorial_conditions=[], split_by_block_num=False):\n",
    "    \"\"\"\n",
    "    Calculates the average accuracy for given test trials based on specified conditions.\n",
    "    \n",
    "    This function computes the mean accuracy for a given set of test trials. It allows for grouping\n",
    "    by a single condition or multiple factorial conditions. The option to further split by block number\n",
    "    is also available. The accuracy is determined by averaging the values in the `correct_trial_col`.\n",
    "    \n",
    "    Input:\n",
    "      df: DataFrame containing task data for a specific task for a single subject.\n",
    "      correct_trial_col (optional): Name of the column indicating correct trials (default is 'correct_trial').\n",
    "      condition_col (optional): Name of the main condition column for grouping (default is 'condition').\n",
    "      test_trial (optional): Specifies the trial type to be considered for accuracy calculation (default is 'test_trial').\n",
    "      factorial_condition (optional): Boolean flag indicating if factorial conditions should be used for grouping (default is False).\n",
    "      factorial_conditions (optional): List of columns to be used for factorial grouping (default is an empty list).\n",
    "      split_by_block_num (optional): Boolean flag to determine if results should be split by block number (default is False).\n",
    "      \n",
    "    Output:\n",
    "      Prints the average accuracy grouped by the specified conditions.\n",
    "    \"\"\"\n",
    "   \n",
    "    test_trials__df = df[df['trial_id'] == test_trial]\n",
    "    \n",
    "    if factorial_condition:\n",
    "        grouping_columns = factorial_conditions\n",
    "    else:\n",
    "        grouping_columns = [condition_col]\n",
    "    \n",
    "    if split_by_block_num:\n",
    "        grouping_columns.append('block_num')\n",
    "    \n",
    "    accuracy_by_condition = test_trials__df.groupby(grouping_columns)[correct_trial_col].mean()\n",
    "    \n",
    "    return accuracy_by_condition.to_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_cpt__json = './data/ax_cpt_rdoc_23-10-25-16:51.json'\n",
    "cued_ts__json = './data/cued_task_switching_rdoc_23-10-25-17:36.json'\n",
    "flanker__json = './data/flanker_rdoc_23-10-25-18:02.json'\n",
    "go_nogo__json = './data/go_nogo_rdoc_23-10-25-18:20.json'\n",
    "n_back__json = './data/n_back_rdoc_23-10-25-18:33.json' \n",
    "span__json = './data/span_rdoc__behavioral_23-10-25-21:45.json' \n",
    "spatial_ts__json = './data/spatial_task_switching_rdoc_23-10-25-20:33.json'\n",
    "spatial_cueing__json = './data/spatial_cueing_rdoc_23-10-25-20:56.json'\n",
    "stroop__json = './data/stroop_rdoc_23-10-25-18:44.json'\n",
    "stop_signal__json = './data/stop_signal_rdoc_23-10-25-19:22.json'\n",
    "visual_search__json = './data/visual_search_rdoc_23-10-25-19:10.json'\n",
    "\n",
    "single_subject__json = {\n",
    "    'ax_cpt': ax_cpt__json,\n",
    "    'cued_ts': cued_ts__json,\n",
    "    'flanker': flanker__json,\n",
    "    'go_nogo': go_nogo__json,\n",
    "    'n_back': n_back__json,\n",
    "    'span': span__json,\n",
    "    'spatial_ts': spatial_ts__json,\n",
    "    'spatial_cueing': spatial_cueing__json,\n",
    "    'stroop': stroop__json,\n",
    "    'stop_signal': stop_signal__json,\n",
    "    'visual_search': visual_search__json\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_subject__json = './data/real_subjects/subject_5ca452ed-f913-40ca-8ae6-327119b26220_2023.11.05.201422.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def organize_data(file):\n",
    "    with open(file, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    data_dict = {}\n",
    "    \n",
    "    for task, task_data in data.items():\n",
    "        for task_item in task_data:\n",
    "            sub_id = task_item['subject']\n",
    "            dict_obj = ast.literal_eval(task_item['data'])\n",
    "            \n",
    "            # Check if 'trialdata' exists in dict_obj\n",
    "            if 'trialdata' not in dict_obj:\n",
    "                continue  # Skip to the next iteration if 'trialdata' is not present\n",
    "            \n",
    "            # Check the type of 'trialdata'\n",
    "            if isinstance(dict_obj['trialdata'], str):\n",
    "                trial_data = json.loads(dict_obj['trialdata'])\n",
    "            else:\n",
    "                trial_data = dict_obj['trialdata']\n",
    "            \n",
    "            single_sub_df = pd.DataFrame(trial_data)\n",
    "            \n",
    "            if sub_id not in data_dict:\n",
    "                data_dict[sub_id] = {}\n",
    "            \n",
    "            if task not in data_dict[sub_id]:\n",
    "                data_dict[sub_id][task] = []\n",
    "\n",
    "            data_dict[sub_id][task].append(single_sub_df)\n",
    "\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = organize_data(real_subject__json)\n",
    "real_subject__feedback = {}\n",
    "real_subject__accuracies = {}\n",
    "real_subject__rts = {}\n",
    "real_subject__omissions = {}\n",
    "\n",
    "for sub in data_dict:\n",
    "    for task__name in data_dict[sub]:\n",
    "        real_subject__feedback[task__name] = {}\n",
    "        real_subject__accuracies[task__name] = {}\n",
    "        real_subject__rts[task__name] = {}\n",
    "        real_subject__omissions[task__name] = {}\n",
    "\n",
    "        if task__name == 'race_ethnicity_RMR_survey__rdoc':\n",
    "            continue\n",
    "        if task__name == 'ax_cpt_rdoc':\n",
    "            task__array = data_dict[sub][task__name]\n",
    "            for index, task__df in enumerate(task__array):\n",
    "                attention_check_accuracy = calculate_attention_check_accuracy(task__df)\n",
    "                omissions = calculate_omission_rate(task__df, test_trial='test_probe')\n",
    "                accuracies = calculate_average_accuracy(task__df, test_trial='test_probe')\n",
    "                rts = calculate_average_rt(task__df, test_trial='test_probe')\n",
    "                feedbacks = feedback_generator('ax_cpt',  accuracies=accuracies, rts=rts, omissions=omissions,attention_check_accuracy=attention_check_accuracy)\n",
    "                real_subject__feedback[task__name][index] = feedbacks\n",
    "                real_subject__accuracies[task__name][index] = accuracies\n",
    "                real_subject__rts[task__name][index] = rts\n",
    "                real_subject__omissions[task__name][index] = omissions\n",
    "        elif task__name == 'cued_ts_rdoc':\n",
    "            task__array = data_dict[sub][task__name]\n",
    "            for index, task__df in enumerate(task__array):\n",
    "                attention_check_accuracy = calculate_attention_check_accuracy(task__df)\n",
    "                accuracies = calculate_average_accuracy(task__df, factorial_condition=True, factorial_conditions=['cue_condition', 'task_condition']) # need to use factorial for cue_condition and task_condition since separate cols\n",
    "                rts = calculate_average_rt(task__df, factorial_condition=True, factorial_conditions=['cue_condition', 'task_condition'])\n",
    "                omissions = calculate_omission_rate(task__df, factorial_condition=True, factorial_conditions=['cue_condition', 'task_condition'])\n",
    "                feedbacks = feedback_generator('cued_ts',  accuracies=accuracies, rts=rts, omissions=omissions, attention_check_accuracy=attention_check_accuracy)\n",
    "                real_subject__feedback[task__name][index] = feedbacks\n",
    "                real_subject__accuracies[task__name][index] = accuracies\n",
    "                real_subject__rts[task__name][index] = rts\n",
    "                real_subject__omissions[task__name][index] = omissions\n",
    "        elif task__name == 'flanker_rdoc':\n",
    "            task__array = data_dict[sub][task__name]\n",
    "            for index, task__df in enumerate(task__array):\n",
    "                attention_check_accuracy = calculate_attention_check_accuracy(task__df)\n",
    "                omissions = calculate_omission_rate(task__df)\n",
    "                accuracies = calculate_average_accuracy(task__df)\n",
    "                rts = calculate_average_rt(task__df)\n",
    "                feedbacks = feedback_generator('flanker',  accuracies=accuracies, rts=rts, omissions=omissions, attention_check_accuracy=attention_check_accuracy)\n",
    "                real_subject__feedback[task__name][index] = feedbacks\n",
    "                real_subject__accuracies[task__name][index] = accuracies\n",
    "                real_subject__rts[task__name][index] = rts\n",
    "                real_subject__omissions[task__name][index] = omissions\n",
    "        elif task__name == 'go_nogo_rdoc':\n",
    "            task__array = data_dict[sub][task__name]\n",
    "            for index, task__df in enumerate(task__array):\n",
    "                attention_check_accuracy = calculate_attention_check_accuracy(task__df)\n",
    "                omissions = calculate_omission_rate(task__df,is_go_no_go=True)\n",
    "                accuracies = calculate_average_accuracy(task__df)\n",
    "                rts = calculate_average_rt(task__df)\n",
    "                feedbacks = feedback_generator('go_nogo',  accuracies=accuracies, rts=rts, omissions=omissions, attention_check_accuracy=attention_check_accuracy)\n",
    "                real_subject__feedback[task__name][index] = feedbacks\n",
    "                real_subject__accuracies[task__name][index] = accuracies\n",
    "                real_subject__rts[task__name][index] = rts\n",
    "                real_subject__omissions[task__name][index] = omissions\n",
    "        elif task__name == 'n_back_rdoc':\n",
    "            task__array = data_dict[sub][task__name]\n",
    "            for index, task__df in enumerate(task__array):\n",
    "                attention_check_accuracy = calculate_attention_check_accuracy(task__df)\n",
    "                accuracies = calculate_average_accuracy(task__df, condition_col='delay') # need to use delay instead of 'match' , 'mismatch' condition \n",
    "                rts = calculate_average_rt(task__df, condition_col='delay') # need to use delay instead of 'match' , 'mismatch' condition \n",
    "                omissions = calculate_omission_rate(task__df, condition_col='delay')\n",
    "                feedbacks = feedback_generator('n_back',  accuracies=accuracies, rts=rts, omissions=omissions, attention_check_accuracy=attention_check_accuracy)\n",
    "                real_subject__feedback[task__name][index] = feedbacks\n",
    "                real_subject__accuracies[task__name][index] = accuracies\n",
    "                real_subject__rts[task__name][index] = rts\n",
    "                real_subject__omissions[task__name][index] = omissions\n",
    "        elif task__name == 'span_rdoc__behavioral':\n",
    "            task__array = data_dict[sub][task__name]\n",
    "            for index, task__df in enumerate(task__array):\n",
    "                attention_check_accuracy = calculate_attention_check_accuracy(task__df)\n",
    "                accuracies = calculate_average_accuracy(task__df, test_trial='test_response')\n",
    "                rts = calculate_average_rt(task__df, test_trial='test_inter-stimulus', correct_trial_col='correct_response')\n",
    "                omissions = calculate_omission_rate__span(task__df) # need something different for omissions since no response in test_response is [] and incomplete is [].length < 4\n",
    "                feedbacks = feedback_generator('span', accuracies=accuracies, rts=rts, omissions=omissions, attention_check_accuracy=attention_check_accuracy)\n",
    "                real_subject__feedback[task__name][index] = feedbacks\n",
    "                real_subject__accuracies[task__name][index] = accuracies\n",
    "                real_subject__rts[task__name][index] = rts\n",
    "                real_subject__omissions[task__name][index] = omissions\n",
    "        elif task__name == 'spatial_ts_rdoc':\n",
    "            task__array = data_dict[sub][task__name]\n",
    "            for index, task__df in enumerate(task__array):\n",
    "                attention_check_accuracy = calculate_attention_check_accuracy(task__df)\n",
    "                omissions = calculate_omission_rate(task__df)\n",
    "                accuracies = calculate_average_accuracy(task__df)\n",
    "                rts = calculate_average_rt(task__df)\n",
    "                feedbacks = feedback_generator('spatial_ts',  accuracies=accuracies, rts=rts, omissions=omissions, attention_check_accuracy=attention_check_accuracy)\n",
    "                real_subject__feedback[task__name][index] = feedbacks\n",
    "                real_subject__accuracies[task__name][index] = accuracies\n",
    "                real_subject__rts[task__name][index] = rts\n",
    "                real_subject__omissions[task__name][index] = omissions\n",
    "        elif task__name == 'spatial_cueing_rdoc':\n",
    "            task__array = data_dict[sub][task__name]\n",
    "            for index, task__df in enumerate(task__array):\n",
    "                attention_check_accuracy = calculate_attention_check_accuracy(task__df)\n",
    "                omissions = calculate_omission_rate(task__df)\n",
    "                accuracies = calculate_average_accuracy(task__df)\n",
    "                rts = calculate_average_rt(task__df)\n",
    "                feedbacks = feedback_generator('spatial_cueing',  accuracies=accuracies, rts=rts, omissions=omissions, attention_check_accuracy=attention_check_accuracy)\n",
    "                real_subject__feedback[task__name][index] = feedbacks\n",
    "                real_subject__accuracies[task__name][index] = accuracies\n",
    "                real_subject__rts[task__name][index] = rts\n",
    "                real_subject__omissions[task__name][index] = omissions\n",
    "        elif task__name == 'stroop_rdoc_rdoc':\n",
    "            task__array = data_dict[sub][task__name]\n",
    "            for index, task__df in enumerate(task__array):\n",
    "                attention_check_accuracy = calculate_attention_check_accuracy(task__df)\n",
    "                omissions = calculate_omission_rate(task__df)\n",
    "                accuracies = calculate_average_accuracy(task__df)\n",
    "                rts = calculate_average_rt(task__df)\n",
    "                feedbacks = feedback_generator('stroop',  accuracies=accuracies, rts=rts, omissions=omissions, attention_check_accuracy=attention_check_accuracy)\n",
    "                real_subject__feedback[task__name][index] = feedbacks\n",
    "                real_subject__accuracies[task__name][index] = accuracies\n",
    "                real_subject__rts[task__name][index] = rts\n",
    "                real_subject__omissions[task__name][index] = omissions\n",
    "        elif task__name == 'stop_signal_rdoc':\n",
    "            task__array = data_dict[sub][task__name]\n",
    "            for index, task__df in enumerate(task__array):\n",
    "                attention_check_accuracy = calculate_attention_check_accuracy(task__df)\n",
    "                stopping_data = get_stopping_data(task__df)\n",
    "                feedbacks = feedback_generator('stop_signal', stopping_data=stopping_data, attention_check_accuracy=attention_check_accuracy)\n",
    "                real_subject__feedback[task__name][index] = feedbacks\n",
    "                real_subject__accuracies[task__name][index] = stopping_data\n",
    "                real_subject__rts[task__name][index] = stopping_data\n",
    "                real_subject__omissions[task__name][index] = stopping_data\n",
    "        elif task__name == 'visual_search_rdoc':\n",
    "            task__array = data_dict[sub][task__name]\n",
    "            for index, task__df in enumerate(task__array):\n",
    "                attention_check_accuracy=attention_check_accuracy\n",
    "                accuracies = calculate_average_accuracy(task__df, factorial_condition=True, factorial_conditions=['condition', 'num_stimuli']) # need to use factorial for load and feature/conjunction\n",
    "                rts = calculate_average_rt(task__df, factorial_condition=True, factorial_conditions=['condition', 'num_stimuli'])\n",
    "                omissions = calculate_omission_rate(task__df, factorial_condition=True, factorial_conditions=['condition', 'num_stimuli'])\n",
    "                feedbacks = feedback_generator('visual_search',  accuracies=accuracies, rts=rts, omissions=omissions, attention_check_accuracy=attention_check_accuracy)\n",
    "                real_subject__feedback[task__name][index] = feedbacks\n",
    "                real_subject__accuracies[task__name][index] = accuracies\n",
    "                real_subject__rts[task__name][index] = rts\n",
    "                real_subject__omissions[task__name][index] = omissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_string(d):\n",
    "    # Convert dictionary to string without enclosing quotes\n",
    "    return ', '.join([f'{k}: {v}' for k, v in d.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_subject__accuracies\n",
    "\n",
    "with open('tasks_acc.csv', 'w', newline='') as csvfile:\n",
    "    # Create a CSV writer object\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    \n",
    "    # Write the header\n",
    "    csvwriter.writerow(['task', 'index', 'values'])\n",
    "    \n",
    "    # Iterate over the dictionary items\n",
    "    for task, task_data in real_subject__accuracies.items():\n",
    "        if task_data:  # Only proceed if there is data\n",
    "            for index, values in task_data.items():\n",
    "                # Convert the nested dictionary to a string using the defined function\n",
    "                values_str = dict_to_string(values)\n",
    "                # Write the task, index, and values string to CSV\n",
    "                csvwriter.writerow([task, index, values_str])\n",
    "        else:\n",
    "            # If there is no data, write the task with empty columns for index and values\n",
    "            csvwriter.writerow([task, '', ''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tasks_rts.csv', 'w', newline='') as csvfile:\n",
    "    # Create a CSV writer object\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    \n",
    "    # Write the header\n",
    "    csvwriter.writerow(['task', 'index', 'values'])\n",
    "    \n",
    "    # Iterate over the dictionary items\n",
    "    for task, task_data in real_subject__rts.items():\n",
    "        if task_data:  # Only proceed if there is data\n",
    "            for index, values in task_data.items():\n",
    "                # Convert the nested dictionary to a string using the defined function\n",
    "                values_str = dict_to_string(values)\n",
    "                # Write the task, index, and values string to CSV\n",
    "                csvwriter.writerow([task, index, values_str])\n",
    "        else:\n",
    "            # If there is no data, write the task with empty columns for index and values\n",
    "            csvwriter.writerow([task, '', ''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tasks_omissions.csv', 'w', newline='') as csvfile:\n",
    "    # Create a CSV writer object\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    \n",
    "    # Write the header\n",
    "    csvwriter.writerow(['task', 'index', 'values'])\n",
    "    \n",
    "    # Iterate over the dictionary items\n",
    "    for task, task_data in real_subject__omissions.items():\n",
    "        if task_data:  # Only proceed if there is data\n",
    "            for index, values in task_data.items():\n",
    "                # Convert the nested dictionary to a string using the defined function\n",
    "                values_str = dict_to_string(values)\n",
    "                # Write the task, index, and values string to CSV\n",
    "                csvwriter.writerow([task, index, values_str])\n",
    "        else:\n",
    "            # If there is no data, write the task with empty columns for index and values\n",
    "            csvwriter.writerow([task, '', ''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback__all_tasks = {}\n",
    "\n",
    "for task__name in single_subject__df:\n",
    "    task__df = single_subject__df[task__name]\n",
    "    if task__name == 'ax_cpt':\n",
    "        attention_check_accuracy = calculate_attention_check_accuracy(task__df)\n",
    "        omissions = calculate_omission_rate(task__df, test_trial='test_probe')\n",
    "        accuracies = calculate_average_accuracy(task__df, test_trial='test_probe')\n",
    "        rts = calculate_average_rt(task__df, test_trial='test_probe')\n",
    "        feedbacks = feedback_generator('ax_cpt',  accuracies=accuracies, rts=rts, omissions=omissions,attention_check_accuracy=attention_check_accuracy)\n",
    "    elif task__name == 'cued_ts':\n",
    "        attention_check_accuracy = calculate_attention_check_accuracy(task__df)\n",
    "        accuracies = calculate_average_accuracy(task__df, factorial_condition=True, factorial_conditions=['cue_condition', 'task_condition']) # need to use factorial for cue_condition and task_condition since separate cols\n",
    "        rts = calculate_average_rt(task__df, factorial_condition=True, factorial_conditions=['cue_condition', 'task_condition'])\n",
    "        omissions = calculate_omission_rate(task__df, factorial_condition=True, factorial_conditions=['cue_condition', 'task_condition'])\n",
    "        feedbacks = feedback_generator('cued_ts',  accuracies=accuracies, rts=rts, omissions=omissions, attention_check_accuracy=attention_check_accuracy)\n",
    "    elif task__name == 'flanker':\n",
    "        attention_check_accuracy = calculate_attention_check_accuracy(task__df)\n",
    "        omissions = calculate_omission_rate(task__df)\n",
    "        accuracies = calculate_average_accuracy(task__df)\n",
    "        rts = calculate_average_rt(task__df)\n",
    "        feedbacks = feedback_generator('flanker',  accuracies=accuracies, rts=rts, omissions=omissions, attention_check_accuracy=attention_check_accuracy)\n",
    "    elif task__name == 'go_nogo':\n",
    "        attention_check_accuracy = calculate_attention_check_accuracy(task__df)\n",
    "        omissions = calculate_omission_rate(task__df)\n",
    "        accuracies = calculate_average_accuracy(task__df)\n",
    "        rts = calculate_average_rt(task__df)\n",
    "        feedbacks = feedback_generator('go_nogo',  accuracies=accuracies, rts=rts, omissions=omissions, attention_check_accuracy=attention_check_accuracy)\n",
    "    elif task__name == 'n_back':\n",
    "        attention_check_accuracy = calculate_attention_check_accuracy(task__df)\n",
    "        accuracies = calculate_average_accuracy(task__df, condition_col='delay') # need to use delay instead of 'match' , 'mismatch' condition \n",
    "        rts = calculate_average_rt(task__df, condition_col='delay') # need to use delay instead of 'match' , 'mismatch' condition \n",
    "        omissions = calculate_omission_rate(task__df, condition_col='delay')\n",
    "        feedbacks = feedback_generator('n_back',  accuracies=accuracies, rts=rts, omissions=omissions, attention_check_accuracy=attention_check_accuracy)\n",
    "    elif task__name == 'span':\n",
    "        attention_check_accuracy = calculate_attention_check_accuracy(task__df)\n",
    "        accuracies = calculate_average_accuracy(task__df, test_trial='test_response')\n",
    "        rts = calculate_average_rt(task__df, test_trial='test_inter-stimulus', correct_trial_col='correct_response')\n",
    "        omissions = calculate_omission_rate__span(task__df) # need something different for omissions since no response in test_response is [] and incomplete is [].length < 4\n",
    "        feedbacks = feedback_generator('span', accuracies=accuracies, rts=rts, omissions=omissions, attention_check_accuracy=attention_check_accuracy)\n",
    "    elif task__name == 'spatial_ts':\n",
    "        attention_check_accuracy = calculate_attention_check_accuracy(task__df)\n",
    "        omissions = calculate_omission_rate(task__df)\n",
    "        accuracies = calculate_average_accuracy(task__df)\n",
    "        rts = calculate_average_rt(task__df)\n",
    "        feedbacks = feedback_generator('spatial_ts',  accuracies=accuracies, rts=rts, omissions=omissions, attention_check_accuracy=attention_check_accuracy)\n",
    "    elif task__name == 'spatial_cueing':\n",
    "        attention_check_accuracy = calculate_attention_check_accuracy(task__df)\n",
    "        omissions = calculate_omission_rate(task__df)\n",
    "        accuracies = calculate_average_accuracy(task__df)\n",
    "        rts = calculate_average_rt(task__df)\n",
    "        feedbacks = feedback_generator('spatial_cueing',  accuracies=accuracies, rts=rts, omissions=omissions, attention_check_accuracy=attention_check_accuracy)\n",
    "    elif task__name == 'stroop':\n",
    "        attention_check_accuracy = calculate_attention_check_accuracy(task__df)\n",
    "        omissions = calculate_omission_rate(task__df)\n",
    "        accuracies = calculate_average_accuracy(task__df)\n",
    "        rts = calculate_average_rt(task__df)\n",
    "        feedbacks = feedback_generator('stroop',  accuracies=accuracies, rts=rts, omissions=omissions, attention_check_accuracy=attention_check_accuracy)\n",
    "    elif task__name == 'stop_signal':\n",
    "        attention_check_accuracy = calculate_attention_check_accuracy(task__df)\n",
    "        stopping_data = get_stopping_data(task__df)\n",
    "        feedbacks = feedback_generator('stop_signal', stopping_data=stopping_data, attention_check_accuracy=attention_check_accuracy)\n",
    "    elif task__name == 'visual_search':\n",
    "        attention_check_accuracy=attention_check_accuracy\n",
    "        accuracies = calculate_average_accuracy(task__df, factorial_condition=True, factorial_conditions=['condition', 'num_stimuli']) # need to use factorial for load and feature/conjunction\n",
    "        rts = calculate_average_rt(task__df, factorial_condition=True, factorial_conditions=['condition', 'num_stimuli'])\n",
    "        omissions = calculate_omission_rate(task__df, factorial_condition=True, factorial_conditions=['condition', 'num_stimuli'])\n",
    "        feedbacks = feedback_generator('visual_search',  accuracies=accuracies, rts=rts, omissions=omissions, attention_check_accuracy=attention_check_accuracy)\n",
    "    feedback__all_tasks[task__name] = feedbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback__all_tasks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
